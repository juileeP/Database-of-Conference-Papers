use jpata;
update Conference_Paper set abstract = "There is an increasing need of complementing the information available for the analysis of biological systems in Systems Biology and Genomics projects. A need that makes interesting the integration of information directly extracted from textual sources using Information Extraction and Text Mining approaches. My group has been working in developing Text Mining approaches and in their integration in large-scale projects together with other experimental and bioinformatics methods. In this occasion I will present the developments related with the characterization of the human mitotic spindle apparatus, developed in the context of the ENFIN NoE. For these, and other, applications it is crucial to have an accurate estimation of the capacity of the current Text Mining systems. The BioCreative II challenge organized by CNIO, MITRE and NCBI in collaboration with the MINT and INTACT databases (http://biocreative.sourceforge.net, Genome Biology, August 2008 Special Issue) provides such an overview. BioCreative II was in two task: 1) gene name identification and normalization, where many systems were able to achieve a consistent 80% balance precision / recall. And 2) protein interaction detection that was divided in four sub-tasks: a) ranking of publications by their relevance on experimental determination of protein interactions, b) detection of protein interaction partners in text, c) detection of key sentences describing protein interactions, and d) detection of the experimental technique used to determine the interactions. The results were quite good in the categories of publication raking, detection of experimental methods, and highlighting of relevant sentences, while they pointed to persistent problems in the correct normalization of gene/protein names. Furthermore BioCreative has channel the collaboration of several teams for the creation of the first Text Mining meta-server (The BioCreative Meta-server, Leitner et al., Genome Biology 2008 BioCreative special issue). We are working now in the preparation of BioCreative III, with particular focus in fostering the creation of Text Mining systems that can be integrated in Genome analysis pipelines, and contribute effectively to the understanding of complex Biological Systems.” where authorID = 3001;

update Conference_Paper set abstract = "Classification using microarray gene expression data is an important task in bioinformatics. Due to the high dimensionality and small sample size that characterizes microarray data, there has recently been a drive to incorporate any available information in addition to the expression data in the classification process. As a result, much work has begun on selecting biological pathways that are closely related to a clinical outcome of interest using the gene expression data, and incorporating this pathway information opens up new avenues for classification. As opposed to previous approaches that consider individual genes as features, we propose a new approach that treats biological pathways as features. Each pathway found to be significantly related to an outcome of interest is treated as a feature, and is mapped to a feature value. We define several methods for mapping pathways to features, and compare the performance of several classifiers using our feature transformations to that of the classifiers using individual genes as features for different feature selection methods." where authorID = 3002;

update Conference_Paper set abstract = ” One of the most challenging problems in mining gene expression data is to identify how the expression of any particular gene affects the expression of other genes. To elucidate the relationships between genes, an association rule mining (ARM) method has been applied to microarray gene expression data. A conventional ARM method, however, has a limit on extracting temporal dependencies between genes, though the temporal information is indispensable to discover underlying regulation mechanisms in biological pathways. In this paper, therefore, we propose a novel method, referred to as temporal association rule mining (TARM), which can extract temporal dependencies among related genes. A temporal association rule has the form [gene A ↑, gene B↓] → (7 min)[gene C], which represents that high expression level of gene A and significant repression of gene B followed by significant expression of gene C after 7 minutes. The proposed TARM method is tested with Saccharomyces cerevisiae cell cycle time-series microarray gene expression data set. In the parameter fitting phase of TARM, the best parameter set [threshold = ±0.8, support cutoff = 3 transactions, confidence cutoff = 90%], which extracted the most number of correct associations in KEGG cell cycle pathway, has been chosen for rule mining phase. Furthermore, comparing the precision scores of TARM (0.38) and Bayesian network (0.16), TARM method showed better accuracy. With the best parameter set, numbers of temporal association rules with five transcriptional time delays (0, 7, 14, 21, 28 minutes) are extracted from gene expression data of 799 genes which are pre-identified cell cycle relevant genes, while comparably small number of rules are extracted from random shuffled gene expression data of 799 genes. From the extracted temporal association rules, associated genes which play same role of biological processes within short transcriptional time delay and some temporal dependencies between genes with specific biological processes are identified.” where authorID = 3003;

update Conference_Paper set abstract = ” We are building the Pharmacogenetics & Pharmacogenomics Knowledgebase (PharmGKB, http://www.pharmgkb.org/) with the goal of cataloguing all knowledge about how genetic variation impacts drug response phenotypes. PharmGKB stores primary data (genotype and phenotype data) as well as more distilled knowledge in the form of pathway diagrams, annotated summaries of very important pharmacogenes (VIP genes), and annotated literature. The literature annotation efforts include both manual curation by trained curators and automatic information extraction. In this talk, I will discuss three projects relevant to our efforts in literature curation: The Pharmspresso project is a simple rule-based system for extracting mentions of gene, drug, disease and polymorphism interactions from text. It is based on the Textpresso system developed at Caltech, but adds specific rules about human drugs, genes and phenotypes. The initial version of Pharmspresso had good performance, but suffered from false positive extractions, and so we have been working to improve the performance, while maintaining as much generality as possible. Pharmspresso is available athttp://pharmspresso.stanford.edu/ The PGxPipeline project builds on the gene-drug-disease associations mined both manually and automatically to do scientific discovery. A critical bottleneck in pharmacogenetics is identifying genes that are likely to be important for modifying drug response. Unless the full details of drug action and metabolism are understood, any of the ~25,000 human genes could be important for understanding action and metabolism. PgxPipeline is built to accept as input a drug and an indication for use (e.g. pain or high cholesterol). It then uses both information from the literature as well as information about chemical structure to rank order all genes in the human genome with respect to the likelihood that they interact with the drug of interest. In this way, we can prioritize the genes that are most likely to be relevant to the drug. We have found that our rank order lists are useful adjuncts to other independent sources of information, and work best in combination with these.Finally, we have been studying the sites in proteins that bind small molecules (such as drugs) or are important as active sites where the proteins functions occur. We have clustered these sites based on structural similarity to discover new structural motifs associated with protein function. Very often, we have no knowledge of the function of these newly discovered structural motifs, but the literature often has substantial information about the function of the proteins to which these motifs belong. Our final project, then, is focused on gathering the literature associated with proteins that have a common motif, and determining what words/concepts are likely to describe the common functions of these proteins, and therefore be the likely significance of these shared structural motifs.” where authorID = 3004;

update Conference_Paper set abstract = ” Microarray data sets contain expression levels of thousands of genes. The statistical analysis of such data sets is typically performed outside a DBMS with statistical packages or mathematical libraries. In this work, we focus on analyzing them inside the DBMS. This is a difficult problem because microarray data sets have high dimensionality, but small size. First, due to DBMS limitations on a maximum number of columns per table, the data set has to be pivoted and transformed before analysis. More importantly, the correlation matrix on tens of thousands of genes has millions of values. While most high dimensional data sets can be analyzed with the classical PCA method, small, but high dimensional, data sets can only be analyzed with Singular Value Decomposition (SVD). We adapt the Householder tridiagonalization and QR factorization numerical methods to solve SVD inside the DBMS. Since these mathematical methods require many matrix operations, which are hard to express in SQL, query optimizations and efficient UDFs are developed to get good performance. Our proposed techniques achieve processing times comparable with those from the R package, a well-known statistical tool. We experimentally show our methods scale well with high dimensionality.” where authorID = 3005;

update Conference_Paper set abstract = ” Proteins and many other biologically relevant molecules are flexible, and the flexibility of a given molecule is one of its important characteristics. In particular, the degree of global and local flexibility of proteins is an important characteristic of protein small-molecule binding sites. In this article, the binding site comparison problem [31, 11, 20, 30, 24, 33] is presented as a deformable partial surface matching problem [7, 13]. The problem is labeled as partial matching because we seek the best match for a given smaller query site (e.g. a small molecule fragment binding site) with respect to each larger binding site (in the search dataset). The question we address is "how can a given part of a binding site be realistically deformed to obtain the best partial match between that part and a full binding site"? This goal is addressed by optimizing the similarity between the site representations subject to modeling the proteins as kinematic chains. The preliminary results of our implementation (ArtSurf) show that the proposed approach is feasible, and that the implementation gives physically reasonable results which improve the detection of partial binding site similarity.” where authorID = 3006;

update Conference_Paper set abstract = ” Cytosine methylation is a DNA modification that has great impact on the regulation of gene expression and important implications for the biology and health of several living beings, including humans. Bisulfite conversion followed by next generation sequencing (BS-seq) of DNA is the gold standard technique used to detect DNA methylation at single-base resolution on a genome scale through the identification of 5-methylcytosine (5-mC). However, by converting unmethylated cytosines into thymines, BS-seq poses computational challenges to read alignment and aggravates the issue of multiple hits due to the ambiguity raised by the reduced sequence complexity. Here we present ERNE-BS5 (Extended Randomized Numerical alignEr - BiSulfite 5), an aligning program developed to efficiently map BS-treated reads against large genomes (e.g., human). To achieve this goal we have implemented three different ideas: (i) we use a 5-letters alphabet for storing methylation information, (ii) we use a weighted context-aware Hamming distance to identify a T coming from an unmethylated C context, and (iii) we use an iterative process to position multiple-hit reads starting from a preliminary map built using single-hit alignments. The map is corrected and extended at each cycle using the alignments added in the previous iteration. ERNE-BS5 is based on a new improved version of the rNA [20] aligning software with a more efficient core.” where authorID = 3007;

update Conference_Paper set abstract = ” We present ARMiCoRe, a novel approach to a classical bioinformatics problem, viz. multiple sequence alignment (MSA) of gene and protein sequences. Aligning multiple biological sequences is a key step in elucidating evolutionary relationships, annotating newly sequenced segments, and understanding the relationship between biological sequences and functions. Classical MSA algorithms are designed to primarily capture conservations in sequences whereas couplings, or correlated mutations, are well known as an additional important aspect of sequence evolution. (Two sequence positions are coupled when mutations in one are accompanied by compensatory mutations in another). As a result, better exposition of couplings is sometimes one of the reasons for hand-tweaking of MSAs by practitioners. ARMiCoRe introduces a distinctly pattern mining approach to improving MSAs: using frequent episode mining as a foundational basis, we define the notion of a coupled pattern and demonstrate how the discovery and tiling of coupled patterns using a max-flow approach can yield MSAs that are better than conservation-based alignments. Although we were motivated to improve MSAs for the sake of better exposing couplings, we demonstrate that our MSAs are also improvements in terms of traditional metrics of assessment. We demonstrate the effectiveness of ARMiCoRe on a large collection of datasets.” where authorID = 3008;

update Conference_Paper set abstract = ” We present ARMiCoRe, a novel approach to a classical bioinformatics problem, viz. multiple sequence alignment (MSA) of gene and protein sequences. Aligning multiple biological sequences is a key step in elucidating evolutionary relationships, annotating newly sequenced segments, and understanding the relationship between biological sequences and functions. Classical MSA algorithms are designed to primarily capture conservations in sequences whereas couplings, or correlated mutations, are well known as an additional important aspect of sequence evolution. (Two sequence positions are coupled when mutations in one are accompanied by compensatory mutations in another). As a result, better exposition of couplings is sometimes one of the reasons for hand-tweaking of MSAs by practitioners. ARMiCoRe introduces a distinctly pattern mining approach to improving MSAs: using frequent episode mining as a foundational basis, we define the notion of a coupled pattern and demonstrate how the discovery and tiling of coupled patterns using a max-flow approach can yield MSAs that are better than conservation-based alignments. Although we were motivated to improve MSAs for the sake of better exposing couplings, we demonstrate that our MSAs are also improvements in terms of traditional metrics of assessment. We demonstrate the effectiveness of ARMiCoRe on a large collection of datasets.” where authorID = 3009;

update Conference_Paper set abstract = ” Given a signaling network, the target combination identification problem aims to predict efficacious and safe target combinations for treatment of a disease. State-of-the-art in silico methods use Monte Carlo simulated annealing (mcsa) to modify a candidate solution stochastically, and use the Metropolis criterion to accept or reject the proposed modifications. However, such stochastic modifications ignore the impact of the choice of targets and their activities on the combinations therapeutic effect and off-target effects which directly affect the solution quality. In this paper, we present Steroid, a novel method that addresses this limitation by leveraging two additional heuristic criteria to minimize off-target effects and achieve synergy for candidate modification. Specifically, off-target effects measure the unintended response of a signaling network to the target combination and is generally associated with toxicity. Synergy occurs when a pair of targets exerts effects that are greater than the sum of their individual effects, and is generally a beneficial strategy for maximizing effect while minimizing toxicity. Our empirical study on the cancer-related mapk-pi3k network demonstrates the superiority of Steroid in comparison to mcsa-based approaches. Specifically, Steroid is an order of magnitude faster and yet yields biologically relevant synergistic target combinations with significantly lower off-target effects.” where authorID = 3010;

update Conference_Paper set abstract = ”Grid is a new solution to computationally and data intensive computing problems. Since the distributed knowledge discovery process is both data and computational intensive, the Grid is a natural platform for deploying a high performance data mining service. In order to improve the performance of data mining applications, an effective method is task parallelization. Existing mechanisms of data mining parallelization are based on NOW or SMP, it is necessary to develop new parallel mechanism for grid feature. In this paper, we present a framework for high performance DDM applications in Computational Grid environments called Data Mining Grid, with the function for decomposing data mining application into subtasks and then combine those subtasks to form directed acyclic graph. This kind of parallel mechanism decomposes application according to the actual computation power of each node in dynamic Grid environment.” where authorID = 3011;

update Conference_Paper set abstract = ”Image edges are the foundation of image texture and shape figure extraction. In this paper we propose a novel edge detection method based on the self-similarity of fractal compression. We point out that the mean-square-error distance (MSE) of fractal compression can be used to extract edge of fractal image effectively. The self-similarity coefficient between the local range block and the searching domain block is centered at the current pixel being processed, and near-center self-affine transform is applied in local searching process, finally a binary operator is used to threshold its magnitude and produce the edge map of the image. The results of experiments show that the proposed new algorithm for image edge detection is valid and effective. It also has good anti-noise performance..” where authorID = 3012;

update Conference_Paper set abstract = ”In this paper, according to the characteristics of the periodic motion, incorporating the grey prediction, repetitive control and the conventional PID control, a design method of the Grey prediction Repetitive PID control algorithm (GRPID) is presented for the first time. The hybrid control algorithm can estimate unsure parameters and disturbance of system using grey prediction, and compensate control in terms of the prediction results, and this may improve control quality and robustness of repetitive control for controlling periodic motion. The simulation results show that this algorithm has better performances than that of the conventional repetitive control system. It indicates the control method has better application effect for motion taking on a characteristic of periodic motion.” where authorID = 3013;

update Conference_Paper set abstract = ” Web-based Geographic Information System (WebGIS) is a key direction towards GIS development. By analysing of the deficiency in the design of conventional WebGIS, this paper proposes an architecture of the intelligent WebGIS based on Multi-Agent, which has been applied in a agricultural WebGIS. In order to solve the key problem of Agent communication, the paper also presents an Agent communication model, which decreases the working capacity to maintain the communication protocl and enhances the systems adaptability and stability.” where authorID = 3014;

update Conference_Paper set abstract = ” Intrusion detection is one of the core technologies in dynamic security. As an important branch of artificial intelligence, neural networks is a high efficient and parallel, non-linear dynamical system, it possesses characteristics of self-adaptive, self-learning and well expansibility. Aimed at the traditional IDS defects of high rate of false alarm and high rate of missing report, we design the method of IDS based on BP neural networks. For huge data samples, the training of the value of the weight is improved compared with the traditional back-propagation (BP) neural networks and simulation, the results show that the method is efficient.” where authorID = 3015;

update Conference_Paper set abstract = ” The theories of classical association psychology (circa 1750-1900) attempted to explain human thought processes in terms of certain mechanistic forces operating on discrete entities called "sensations," "images," and "ideas." Although these theories have become unfashionable since the turn of the century, due primarily to their ambiguity and the difficulty of experimental verification, and whereas they may never prove adequate for human psychology, it is possible that they may, nevertheless, provide a fruitful basis for some types of artificial intelligence. One method of exploring ramifications of the classical theories is the formulation of an abstract "machine" which constitutes an interpretation of the theories and whose behavior can be examined in any desired detail. In this paper such a machine is partially constructed, and some of its behavioral features and problems are discussed.” where authorID = 3016;

update Conference_Paper set abstract = ” This paper deals with nonlinear programming. In particular, it summarizes a newly developed program suitable for optimization of a computer-controlled process. The program applies probing and constraint-following algorithms which permit solving the optimization problem in difficult cases. These cases include nonlinear or discontinuous objective functions, constraint functions, and nonconvex domains. The program has become known as the "Poor Mans Optimizer," as it requires relatively little storage for program and data. It is applicable to the relatively small digital computers now popular in process control.” where authorID = 3017;

update Conference_Paper set abstract = ” This paper describes a simulation model of a hypothetical business firm. The model was constructed to include not only the accounting and economic factors of costs, profits, sales, units produced, etc., but also psychological and behavioral concepts. Individuals in the firm have aspiration levels, feel pressure, and react in accordance with behavioral theory.The purpose of the model is to study the effects of informational and organizational factors upon the decisions of a business firm. We have had limited knowledge of such variables as: the effects of tardy information, the effects of different distributions of information within the firm, the effects of differing degrees of centralization or decentralization, etc. A comprehensive model, such as the one proposed, is necessary to answer such questions. Eight specific hypotheses involving changes in the organization and information system of the firm were formulated and tested using a factorial experimental design. The results of this experiment demonstrate the usefulness of this model as a research tool.” where authorID = 3018;

update Conference_Paper set abstract = ” The general properties of an associative memory are explained, and their advantages relative to a random access memory discussed. Then, a superconductive mechanization of such a memory is described which is based upon the cross film cryotron. The memory requires 5 cryotrons per bit and 9 cryotrons for a control module associated with each word. Any combination of bits of the word can be used as the key, and any number of records in the memory can be identified and read out as the result of a single association. The speed of various circuits in the memory is approximated and some applications are suggested.” where authorID = 3019;

update Conference_Paper set abstract = ” Simulation of a system by digital computer requires: A model of the system which is intelligible to the student of the system while compatible with the limitations of the computer. Translation of the model to computer code. Movement of the model through time. Recording the performance of the model. SIMPAC, a "simulation package," incorporates coherent techniques and devices for the accomplishment of these objectives: modeling concepts for building a computer-compatible model, a vocabulary for encoding the model, a computer program for moving the model through time and recording its performance, and an output presentation program. A model of a hypothetical business system has been implemented with the first version of SIMPAC for the purpose of studying management controls in a complex system.This paper discusses digital simulation and SIMPAC and introduces modeling concepts which may lead to a set of simulation systems, called 'Muse', which would assemble models of varying complexity from descriptive statements and analyze the models prior to simulation.” where authorID = 3020;

update Conference_Paper set abstract = “String kernel-based machine learning methods have yielded great success in practical tasks of structured/sequential data analysis. They often exhibit state-of-the-art performance on tasks such as document topic elucidation, biological sequence classification, or protein superfamily and fold prediction. However, typical string kernel methods rely on analysis of discrete 1D string data (e.g., DNA or amino acid sequences). This work introduces new 2D kernel methods for sequence data in the form of sequences of feature vectors (as in biological sequence profiles, or sequences of individual amino acid physico-chemical descriptors). On three protein sequence classification tasks proposed 2D kernels show significant 15-20% improvements compared to state-of-the-art sequence classification methods.” where authorID = 3021;

update Conference_Paper set abstract = “Recent work using graph representations for text categorization has shown promising performance over conventional bag-of-words representation of text documents. In this paper we investigate a graph representation of texts for the task of text categorization. In our representation we identify high level concepts extracted from a database of controlled biomedical terms and build a rich graph structure that contains important concepts and relationships. This procedure ensures that graphs are described with a regular vocabulary, leading to increased ease of comparison. We then classify document graphs by applying a set-based graph kernel that is intuitively sensible and able to deal with the disconnectedness of the constructed concept graphs. We compare this approach to standard approaches using non-graph, text-based features. We also do a comparison amongst different kernels that can be used to see which performs better.” where authorID = 3022;

update Conference_Paper set abstract = “High-throughput experimental techniques have made available large datasets of experimentally detected protein-protein interactions. However, experimentally determined protein complexes datasets are not exhaustive nor reliable. A protein complex plays a key role in disease development. Therefore, the identification and characterization of protein complexes involved is crucial to the understanding of the molecular events under normal and abnormal physiological conditions. In this paper, we propose a novel graph mining algorithm to identify protein complexes. The algorithm first checks the quality of the interaction data, then predicts protein complexes based on the concept of weighted clustering coefficient. To demonstrate the effectiveness of our proposed method, we present experimental results on yeast protein interaction data. The level of accuracy achieved is a strong argument in favor of the proposed method. Novel protein complexes were also predicted to assist biologists in their search for protein complexes. The datasets and programs are freely available from http://faculty.uaeu.ac.ae/nzaki/PE-WCC.htm.” where authorID = 3023;

update Conference_Paper set abstract = ”In pharmacology, it is essential to identify the interactions between drug and targets to understand its effects. Supervised learning with Bipartite Local Model (BLM) recently has been shown to be effective for prediction of drug-target interactions by first predicting target proteins of a given known drug, then predicting drugs targeting a known protein. However, this pure "local" model is inapplicable to new drug or target candidates that currently have no known interactions. In this paper, we extend the existing BLM method by integrating a strategy for handling new drug and target candidates. Based on the assumption that similar drugs and targets have similar interaction profiles, we present a simple neighbor-based training data inferring procedure and integrate it into the frame work of BLM. This globalized BLM called bipartite local model with neighbor-based inferring (BLMN) then has an extended functionality for prediction interactions between new drug candidates and new target candidates. Good performance of BLMN has been observed in the experiment of predicting interactions between drugs and four important categories of targets. For the Nuclear Receptors dataset, where there are more chances for the presented strategy to be applied, 20% improvement in terms of AUPR was achieved. This demonstrates the effectiveness of BLMN and its potential in prediction of drug-target interactions.” where authorID = 3024;

update Conference_Paper set abstract = ” Automatically extracting chemical names from text has significant value to biomedical and life science research. A major barrier in this task is the difficulty of getting a sizable good quality training set to train a reliable entity extraction model. Leveraging the well-studied random text generation techniques based on formal grammars, we explore the idea of automatically creating training sets for the task of chemical named entity extraction. Assuming the availability of an incomplete list of chemical names, we are able to generate well-controlled, random, yet realistic chemical-like training documents. Compared to state-of-the-art models learned from manually labeled data and rule-based systems using real-world data, our solutions show comparable or better results, with least human effort.” where authorID = 3025;





